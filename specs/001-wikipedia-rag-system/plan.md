# Implementation Plan: Wikipedia RAG System

**Branch**: `001-wikipedia-rag-system` | **Date**: 2025-11-02 | **Spec**: [spec.md](./spec.md)
**Input**: Feature specification from `/specs/001-wikipedia-rag-system/spec.md`

**Note**: This plan is generated by the `/speckit.plan` command based on research and design artifacts.

## Summary

The Wikipedia RAG System enables users to create knowledge bases from Wikipedia articles and interact with them through a chat interface powered by a local LLM running on AWS infrastructure. The system consists of two primary workflows: (1) RAG construction from Wikipedia URLs, and (2) natural language chat queries against indexed content. All LLM inference occurs within AWS (no external API calls), aligning with privacy and cost efficiency goals.

**Technical Approach**:
- **LLM Platform**: Ollama with Llama 3.2 3B Instruct (Q4 quantization, ~1.8GB)
- **Vector Database**: FAISS (file-based, S3-compatible)
- **Embedding Model**: all-MiniLM-L6-v2 (384 dimensions, 90MB)
- **Infrastructure**: AWS CDK, t4g.medium EC2, Lambda (ARM64), S3, CloudFront, API Gateway
- **Estimated Cost**: $25.96/month (baseline), $8.83/month (spot instance) - well within $50 budget

## Technical Context

**Language/Version**: Python 3.11+
**Primary Dependencies**: LangChain, Ollama, FAISS, sentence-transformers, FastAPI, wikipedia-api, boto3, AWS CDK
**Storage**: S3 (vector indices, raw content), DynamoDB (optional session metadata), FAISS (in-memory vector search)
**Testing**: pytest, moto (AWS mocking), hypothesis (property testing)
**Target Platform**: AWS (EC2 t4g.medium for LLM, Lambda ARM64 for orchestration, S3 + CloudFront for frontend)
**Project Type**: Web application (backend API + frontend UI)
**Performance Goals**:
  - RAG build: <60 seconds for 50KB articles (SC-001)
  - Query response: <2 seconds first token (SC-002)
  - Concurrent RAG builds: 10+ without degradation (SC-003)
**Constraints**:
  - Development cost: <$50/month (SC-006)
  - No external LLM APIs (Privacy First principle)
  - Latency target: <2s per query (SC-002)
**Scale/Scope**:
  - Initial: 100 RAG sessions, 1000 queries/month
  - Target: 1000 sessions, 10K queries/month within budget

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

### Principle I: Modularity ✅

**Status**: PASSED

**Evidence**:
- RAG construction implemented as independent Lambda function (`rag-builder-api`)
- Chat functionality implemented as separate Lambda function (`chat-api`)
- LLM inference service runs on dedicated EC2 instance
- Frontend is static site hosted on S3/CloudFront, decoupled from backend
- Each component can be deployed, scaled, and updated independently

**Verification**: API contracts in `contracts/` directory show clear separation of concerns.

### Principle II: Privacy First ✅

**Status**: PASSED

**Evidence**:
- Ollama deployed on EC2 within AWS VPC (no external API calls)
- All LLM inference happens locally on t4g.medium instance
- No data sent to OpenAI, Anthropic, or any external LLM service
- Wikipedia URLs and chat history remain within AWS infrastructure
- Embedding generation uses local sentence-transformers model

**Verification**: Architecture diagram (see below) shows no external LLM dependencies.

### Principle III: Cost Efficiency ✅

**Status**: PASSED

**Evidence**:
- Baseline configuration: $25.96/month (48% below $50 budget)
- Optimized configuration with spot instances: $8.83/month (82% below budget)
- Serverless Lambda for orchestration (pay-per-use, ~$0.05/month)
- S3 for storage instead of managed databases (~$0.005/month)
- ARM64 architecture (Graviton2) for 20% cost savings on Lambda

**Verification**: Cost breakdown in `research.md` shows detailed calculations.

### Principle IV: Infrastructure as Code ✅

**Status**: PASSED

**Evidence**:
- AWS CDK (Python) selected for infrastructure definitions
- All infrastructure code in `infrastructure/` directory (same repo as application code)
- EC2, Lambda, S3, API Gateway, CloudFront defined as CDK stacks
- No manual AWS console configuration required
- Infrastructure changes reviewed through Git pull requests

**Verification**: `quickstart.md` includes CDK deployment instructions.

### Principle V: Observability ✅

**Status**: PASSED

**Evidence**:
- CloudWatch Logs for all Lambda functions
- CloudWatch Metrics for LLM inference latency (custom metric)
- RAG build status tracking (processing → ready → failed)
- Error tracking for Wikipedia fetch failures and LLM timeouts
- Cost monitoring with AWS Budgets alerts ($40, $45, $50 thresholds)

**Verification**: Monitoring strategy documented in `research.md` section 6.

### Technology Stack Compliance ✅

**Status**: PASSED

All mandated technologies from Constitution are used:

- ✅ Cloud Platform: AWS (EC2, Lambda, S3, CloudFront, API Gateway)
- ✅ Backend: Python 3.11+, LangChain, Ollama (local LLM), FAISS (vector DB)
- ✅ Frontend: Svelte (developer choice, meets Constitution option)
- ✅ Data Storage: S3 (no relational database)
- ✅ IaC: AWS CDK (Python)

## Project Structure

### Documentation (this feature)

```text
specs/001-wikipedia-rag-system/
├── plan.md              # This file (/speckit.plan command output)
├── spec.md              # Feature specification (user requirements)
├── research.md          # Phase 0 output (technical decisions)
├── data-model.md        # Phase 1 output (entity definitions)
├── quickstart.md        # Phase 1 output (development setup guide)
├── contracts/           # Phase 1 output (API contracts)
│   ├── rag-builder-api.yaml  # RAG construction API
│   └── chat-api.yaml         # Chat query API
├── checklists/          # Quality validation
│   └── requirements.md  # Spec quality checklist
└── tasks.md             # Phase 2 output (/speckit.tasks command - NOT created yet)
```

### Source Code (repository root)

This is a **web application** with backend API and frontend UI, deployed on AWS.

```text
backend/
├── src/
│   ├── models/
│   │   ├── rag_session.py      # RAG session entity
│   │   ├── text_chunk.py       # Text chunk entity
│   │   ├── query.py            # Query entity
│   │   └── chat_message.py     # Chat message entity
│   ├── services/
│   │   ├── wikipedia_fetcher.py    # Wikipedia scraping + parsing
│   │   ├── embedding_service.py    # all-MiniLM-L6-v2 embeddings
│   │   ├── vector_store.py         # FAISS index management
│   │   ├── llm_service.py          # Ollama LLM client
│   │   ├── rag_builder.py          # RAG construction orchestration
│   │   └── chat_service.py         # Query + retrieval + LLM chain
│   ├── api/
│   │   ├── lambda_handlers/
│   │   │   ├── rag_builder_handler.py  # POST /rag/build Lambda
│   │   │   └── chat_handler.py         # POST /chat/query Lambda
│   │   └── fastapi_app.py      # Local development API (optional)
│   └── utils/
│       ├── s3_client.py        # S3 operations wrapper
│       ├── validation.py       # Input validation
│       └── error_handling.py   # Error response formatting
└── tests/
    ├── contract/              # API contract tests
    │   ├── test_rag_builder_api.py
    │   └── test_chat_api.py
    ├── integration/           # End-to-end tests
    │   ├── test_rag_pipeline.py
    │   └── test_chat_flow.py
    └── unit/                  # Unit tests
        ├── test_wikipedia_fetcher.py
        ├── test_embedding_service.py
        └── test_vector_store.py

frontend/
├── src/
│   ├── components/
│   │   ├── ChatInterface.svelte     # Main chat UI
│   │   ├── WikipediaInput.svelte    # URL input form
│   │   ├── MessageList.svelte       # Chat message display
│   │   └── StatusIndicator.svelte   # RAG build status
│   ├── pages/
│   │   ├── Index.svelte             # Landing page
│   │   └── Chat.svelte              # Chat page
│   ├── services/
│   │   ├── api_client.js            # Backend API client
│   │   └── sse_handler.js           # Server-Sent Events for streaming
│   └── stores/
│       ├── session_store.js         # RAG session state
│       └── chat_store.js            # Chat history state
└── tests/
    └── e2e/                         # End-to-end UI tests

infrastructure/
├── app.py                    # CDK app entry point
├── stacks/
│   ├── compute_stack.py      # EC2 instance for Ollama
│   ├── lambda_stack.py       # Lambda functions + API Gateway
│   ├── storage_stack.py      # S3 buckets
│   └── frontend_stack.py     # CloudFront + S3 frontend hosting
└── cdk.json                  # CDK configuration

scripts/
├── setup_ollama.sh           # EC2 user-data script (Ollama installation)
└── deploy.sh                 # Deployment automation

docs/
├── architecture.md           # Architecture diagrams
└── api_usage.md              # API usage examples

.github/
└── workflows/
    ├── test.yml              # CI: Run tests
    └── deploy.yml            # CD: Deploy to AWS

requirements.txt              # Python dependencies
pyproject.toml                # Poetry configuration (optional)
.env.example                  # Environment variables template
README.md                     # Project README
```

**Structure Decision**:

We selected **Option 2: Web application** structure because:
1. The system has distinct backend (API) and frontend (UI) components
2. Backend runs on AWS Lambda + EC2, frontend runs as static site on S3/CloudFront
3. Clear separation enables independent development and deployment
4. Aligns with Constitution Principle I (Modularity)

## Architecture Diagram

```
┌─────────────────────────────────────────────────────────────────┐
│                          User Browser                            │
└───────────────────────┬─────────────────────────────────────────┘
                        │
                        ▼
┌─────────────────────────────────────────────────────────────────┐
│                     CloudFront CDN                               │
│              (Static Frontend - Svelte)                          │
└───────────────────────┬─────────────────────────────────────────┘
                        │
                        ▼
┌─────────────────────────────────────────────────────────────────┐
│                      API Gateway                                 │
│                   (REST API + SSE)                               │
└────────┬────────────────────────────────────────┬───────────────┘
         │                                        │
         │ POST /rag/build                       │ POST /chat/query
         │ GET /rag/{id}/status                  │ GET /chat/history
         │                                        │
         ▼                                        ▼
┌─────────────────────┐                 ┌─────────────────────────┐
│  Lambda: RAG Builder│                 │  Lambda: Chat Handler   │
│   (ARM64, 1024MB)   │                 │    (ARM64, 512MB)       │
└──────┬──────────────┘                 └───────┬─────────────────┘
       │                                        │
       │ 1. Fetch Wikipedia                    │ 1. Load FAISS index
       │ 2. Chunk + Embed                      │ 2. Retrieve chunks
       │ 3. Create FAISS index                 │ 3. Call LLM
       │ 4. Save to S3                         │
       │                                        │
       ▼                                        ▼
┌─────────────────────────────────────────────────────────────────┐
│                         S3 Buckets                               │
│  - FAISS indices (Standard)                                      │
│  - Wikipedia raw content (Standard-IA)                           │
│  - Session metadata (JSON)                                       │
└─────────────────────────────────────────────────────────────────┘
                                                │
                                                │ HTTP calls to LLM
                                                ▼
                                    ┌───────────────────────────┐
                                    │   EC2: t4g.medium         │
                                    │   (Ollama + Llama 3.2 3B) │
                                    │   Private VPC             │
                                    └───────────────────────────┘
```

**Key Flows**:

1. **RAG Construction Flow**:
   - User submits Wikipedia URL via frontend
   - API Gateway → Lambda (RAG Builder)
   - Lambda fetches Wikipedia page (wikipedia-api)
   - Lambda chunks text (RecursiveCharacterTextSplitter)
   - Lambda generates embeddings (all-MiniLM-L6-v2)
   - Lambda creates FAISS index
   - Lambda uploads index to S3
   - Returns session_id to user

2. **Chat Query Flow**:
   - User submits query + session_id via frontend
   - API Gateway → Lambda (Chat Handler)
   - Lambda downloads FAISS index from S3
   - Lambda performs similarity search (retrieve top-k chunks)
   - Lambda calls Ollama on EC2 with context + query
   - Lambda streams LLM response via SSE
   - Returns answer to user

## Complexity Tracking

> **No violations found. Constitution Check passed all principles.**

This section is intentionally empty as there are no constitutional violations to justify. All architectural decisions align with the five core principles:
- Modularity: Separate Lambda functions for RAG building and chat
- Privacy First: Local LLM on EC2, no external APIs
- Cost Efficiency: $25.96/month baseline, $8.83/month optimized
- Infrastructure as Code: AWS CDK for all infrastructure
- Observability: CloudWatch Logs and Metrics

## Phase Summary

### Phase 0: Research ✅ COMPLETED

**Artifacts Created**:
- `research.md` - Technical decision documentation (23KB)

**Key Decisions**:
1. LLM Platform: Ollama (ease of deployment, privacy)
2. Vector Database: FAISS (S3 compatibility, speed)
3. IaC Tool: AWS CDK (Python, AWS-specific)
4. Embedding Model: all-MiniLM-L6-v2 (speed, cost)
5. Wikipedia Scraping: wikipedia-api + BeautifulSoup
6. Cost Optimization: t4g.medium EC2, ARM64 Lambda, spot instances

**Research Status**: All NEEDS CLARIFICATION items resolved.

### Phase 1: Design ✅ COMPLETED

**Artifacts Created**:
- `data-model.md` - Entity definitions with validation rules (20KB)
- `contracts/rag-builder-api.yaml` - OpenAPI spec for RAG construction (22KB)
- `contracts/chat-api.yaml` - OpenAPI spec for chat interface (25KB)
- `quickstart.md` - Development setup guide (20KB)

**Key Deliverables**:
1. **Data Models**: RAG Session, Text Chunk, Query, Chat Message with validation
2. **API Contracts**: 7 endpoints with request/response schemas, error handling
3. **Development Guide**: Local setup, testing, deployment, troubleshooting

**Design Status**: All artifacts validated against spec.md requirements.

### Phase 2: Tasks (NEXT STEP)

**Command**: `/speckit.tasks`

This command will generate `tasks.md` with implementation tasks organized by user story priority (P1 → P2 → P3). Expected task categories:
- Phase 1: Setup (project structure, dependencies)
- Phase 2: Foundational (AWS CDK infrastructure, Ollama deployment)
- Phase 3: User Story 1 - Build RAG (P1, MVP)
- Phase 4: User Story 2 - Chat with RAG (P2)
- Phase 5: User Story 3 - Profile Page Integration (P3)
- Phase N: Polish & Documentation

## Next Steps

1. ✅ **Phase 0 Complete**: Technical research finished, all decisions documented
2. ✅ **Phase 1 Complete**: Data models, API contracts, quickstart guide created
3. ⏭️  **Run `/speckit.tasks`**: Generate implementation task list
4. ⏭️  **Implementation**: Execute tasks in priority order (P1 → P2 → P3)

## Success Criteria Validation

| Criterion | Target | Plan Evidence |
|-----------|--------|---------------|
| SC-001 | RAG build <60s (50KB) | Estimated 10-20s (research.md) |
| SC-002 | Query response <2s | all-MiniLM-L6-v2: 14.7ms/1K tokens, Llama 3.2: 19.92 tokens/sec |
| SC-003 | 10 concurrent builds | Lambda concurrency + t4g.medium capacity |
| SC-004 | 90% relevant responses | FAISS similarity search + Llama 3.2 3B quality |
| SC-005 | No external LLM APIs | Ollama on EC2 (Private VPC) |
| SC-006 | <$50/month | $25.96 baseline, $8.83 spot (research.md) |
| SC-007 | UI loads <3s | S3 + CloudFront static site hosting |

**Status**: All success criteria have feasible implementation paths documented in research and design artifacts.

## Risk Assessment

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| EC2 spot interruption | Medium | Medium | Auto-restart script, state persistence to S3 |
| LLM response latency >2s | Low | Medium | Model optimization (Q4 quantization), ARM CPU tuning |
| Cost overrun | Low | High | AWS Budgets alerts ($40/$45/$50), weekly reviews |
| FAISS index corruption | Low | Medium | S3 versioning enabled, backup indices |
| Wikipedia rate limiting | Low | Low | Respect robots.txt, 1s delay between requests |
| Lambda cold start delays | Medium | Low | Provisioned concurrency for critical functions |

## References

- [Feature Specification](./spec.md)
- [Research Document](./research.md)
- [Data Model](./data-model.md)
- [RAG Builder API Contract](./contracts/rag-builder-api.yaml)
- [Chat API Contract](./contracts/chat-api.yaml)
- [Quickstart Guide](./quickstart.md)
- [Constitution](./../.specify/memory/constitution.md)
